{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"final_data/densenet_valid_embeddings.csv\")\n",
    "train = pd.read_csv(\"final_data/densenet_test_embeddings.csv\", quotechar='\"', on_bad_lines='skip')\n",
    "\n",
    "print(test.columns)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert embeddings from str to list (a bit long for large data sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['embeddings'] = test['embeddings'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['embeddings'] = train['embeddings'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(columns=['path_to_image', 'path_to_dcm'])\n",
    "train = train.drop(columns=['path_to_image', 'path_to_dcm'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rows that were not processed (embeddings = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_size = test.shape[0] \n",
    "\n",
    "# The previous logic with transforming the list to string and filtering on the length of said string is not necessarily stable and misleading.\n",
    "# Let's implement a more explicit test for what we actually care about: \n",
    "\n",
    "test = test[test['embeddings'].apply(type) == list]\n",
    "\n",
    "final_size = test.shape[0] \n",
    "\n",
    "print(f'Number of test removed rows = {initial_size - final_size}')\n",
    "\n",
    "initial_size = train.shape[0] \n",
    "\n",
    "train = train[train['embeddings'].apply(type) == list]\n",
    "\n",
    "final_size = train.shape[0] \n",
    "\n",
    "print(f'Number of train removed rows = {initial_size - final_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Using Random Forest as the base estimator\n",
    "# base_rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# # Multi-output classifier\n",
    "# multi_target_rf = MultiOutputClassifier(base_rf, n_jobs=-1)\n",
    "\n",
    "# # Train the model\n",
    "# multi_target_rf.fit(train_embeddings, y_train)\n",
    "\n",
    "# # Predict on test data\n",
    "# predictions = multi_target_rf.predict(test_embeddings)\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "# print(\"F1 Score:\", f1_score(y_test, predictions, average='micro'))\n",
    "# print(\"Recall:\", recall_score(y_test, predictions, average='micro'))\n",
    "# print(\"Precision:\", precision_score(y_test, predictions, average='micro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = pd.DataFrame(train['embeddings'].tolist(), columns=[f'embedding_{i}' for i in range(1024)])\n",
    "test_embeddings = pd.DataFrame(test['embeddings'].tolist(), columns=[f'embedding_{i}' for i in range(1024)])\n",
    "\n",
    "# Diseases to predict\n",
    "diseases = ['Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia',\n",
    "            'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture']\n",
    "\n",
    "# Labels for train and test\n",
    "y_train = train[diseases]\n",
    "y_test = test[diseases]\n",
    "\n",
    "# Create x_train and x_test\n",
    "x_train = pd.concat([train.reset_index(), train_embeddings], axis=1)\n",
    "x_test =  pd.concat([test.reset_index(), test_embeddings], axis=1)\n",
    "\n",
    "x_train.drop(columns=[\"embeddings\"] + diseases, inplace=True)\n",
    "x_test.drop(columns=[\"embeddings\"] + diseases, inplace=True)\n",
    "\n",
    "# Create some backups:\n",
    "_x_train, _x_test, _y_train, _y_test = x_train.copy(deep=True), x_test.copy(deep=True), y_train.copy(deep=True), y_test.copy(deep=True)\n",
    "# To restore the backups, run:\n",
    "# x_train, x_test, y_train, y_test = _x_train.copy(deep=True), _x_test.copy(deep=True), _y_train.copy(deep=True), _y_test.copy(deep=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = ['Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', \n",
    "            'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n",
    "            'Pleural Effusion', 'Pleural Other', 'Fracture']\n",
    "\n",
    "def train_model(x_train, y_train, x_test, y_test, model, metric_dimensions=[], columns_to_drop=[], n_components=None):\n",
    "    multi_output_model = MultiOutputClassifier(model)\n",
    "    x_test = x_test.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    x_train_subset = x_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "    x_test_subset = x_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Apply PCA if wanted\n",
    "    if n_components:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        x_train_subset = pca.fit_transform(x_train_subset)\n",
    "        x_test_subset = pca.transform(x_test_subset)\n",
    "        print(f\"PCA used with {n_components} components.\")\n",
    "    else:\n",
    "        print(\"PCA not used.\")\n",
    "\n",
    "    metrics_data = {\n",
    "        'Metric Dimension': [],\n",
    "        'Subgroup': [],\n",
    "        'Disease': [],\n",
    "        'Accuracy': [],\n",
    "        'F1 Score': [],\n",
    "        'Recall': [], # = True Positive Rate\n",
    "        'FPR': [],\n",
    "        'Precision': [],\n",
    "        'AUC': [],\n",
    "    }\n",
    "\n",
    "    def calculate_metrics(y_true, y_pred, y_pred_proba=None, disease_label=\"Overall\"):\n",
    "        \"\"\" Helper function to calculate metrics and add them to metrics_data \"\"\"\n",
    "        metrics_data['Metric Dimension'].append(metric_dim)\n",
    "        metrics_data['Subgroup'].append(subgroup)\n",
    "        metrics_data['Disease'].append(disease_label)\n",
    "        metrics_data['Accuracy'].append(accuracy_score(y_true, y_pred))\n",
    "        metrics_data['F1 Score'].append(f1_score(y_true, y_pred, average='micro'))\n",
    "        metrics_data['Recall'].append(recall_score(y_true, y_pred, average='micro'))\n",
    "        metrics_data['Precision'].append(precision_score(y_true, y_pred, average='micro'))\n",
    "        if y_pred_proba is not None:\n",
    "            try:\n",
    "                auc = roc_auc_score(y_true, y_pred_proba)\n",
    "            except ValueError: \n",
    "                auc = None\n",
    "            metrics_data['AUC'].append(auc)\n",
    "        else:\n",
    "            metrics_data['AUC'].append(None)\n",
    "\n",
    "        # Calculate FPR\n",
    "        if isinstance(y_true, pd.Series):  # Single-label case\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            fpr = fp / (fp + tn) if (fp + tn) > 0 else None\n",
    "            metrics_data['FPR'].append(fpr)\n",
    "        else:  # Multi-label case\n",
    "            fprs = []\n",
    "            for c in y_true.columns:\n",
    "                cm = confusion_matrix(y_true[c], y_pred[c])\n",
    "                if cm.shape == (2, 2):  # Ensure binary confusion matrix shape\n",
    "                    tn, fp, fn, tp = cm.ravel()\n",
    "                    fpr = fp / (fp + tn) if (fp + tn) > 0 else None\n",
    "                else:\n",
    "                    fpr = None  # Handle cases where FPR isn't defined\n",
    "                fprs.append(fpr)\n",
    "            \n",
    "            mean_fpr = np.nanmean(fprs)  # Mean FPR across all columns\n",
    "            metrics_data[\"FPR\"].append(mean_fpr)\n",
    "\n",
    "\n",
    "    try:\n",
    "        multi_output_model.fit(x_train_subset, y_train)\n",
    "        y_test_preds = pd.DataFrame(multi_output_model.predict(x_test_subset), columns=diseases)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_test_preds_proba = pd.DataFrame({disease: probs[:, 1] for disease, probs in zip(diseases, multi_output_model.predict_proba(x_test_subset))}) # Dataframe with probabilites \n",
    "        else:\n",
    "            y_test_preds_proba = None\n",
    "\n",
    "        metric_dim, subgroup = \"all\", \"all\"\n",
    "        calculate_metrics(y_test, y_test_preds, y_test_preds_proba)\n",
    "\n",
    "      \n",
    "        \n",
    "        # Calculate metrics for each disease individually\n",
    "        for disease in diseases:\n",
    "            y_true_disease = y_test[disease]\n",
    "            y_pred_disease = y_test_preds[disease]\n",
    "            y_pred_proba_disease = y_test_preds_proba[disease] if y_test_preds_proba is not None else None\n",
    "            calculate_metrics(y_true_disease, y_pred_disease, y_pred_proba_disease, disease_label=disease)\n",
    "    except ValueError:\n",
    "        print(\"Multi-output not supported. Training each disease separately.\")\n",
    "        for disease in diseases:\n",
    "            model.fit(x_train_subset, y_train[disease])\n",
    "            y_test_preds = model.predict(x_test_subset)\n",
    "            y_test_preds_proba = model.predict_proba(x_test_subset)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "            metric_dim, subgroup = \"all\", \"all\"\n",
    "            calculate_metrics(y_test[disease], y_test_preds, y_test_preds_proba, disease_label=disease)\n",
    "\n",
    "    # Calculate metrics for each dimension in `metric_dimensions` - in our case mostly sex, race, and insurance_type\n",
    "    for metric_dim in metric_dimensions:\n",
    "        for subgroup in x_test[metric_dim].unique(): #looks at every unique subgroup e.g. female and male \n",
    "            mask = (x_test[metric_dim] == subgroup) # here we only look at the rows where the subgroup e.g. female is True\n",
    "            x_test_subgroup = x_test_subset[mask]\n",
    "            y_test_subgroup = y_test.loc[mask]\n",
    "            try:\n",
    "                y_test_preds_subgroup = y_test_preds.loc[mask] # filtern of the predictions we made before just with that one subgroup\n",
    "                y_test_preds_proba_subgroup = y_test_preds_proba.loc[mask] if y_test_preds_proba is not None else None # filters also if applicable the y_test_pred_proba for that subgorup and line?\n",
    "                calculate_metrics(y_test_subgroup, y_test_preds_subgroup, y_test_preds_proba_subgroup)\n",
    "                for disease in diseases:\n",
    "                    y_true_subgroup_disease = y_test_subgroup[disease]\n",
    "                    y_pred_subgroup_disease = y_test_preds_subgroup[disease]\n",
    "                    y_pred_proba_subgroup_disease = y_test_preds_proba_subgroup[disease] if y_test_preds_proba_subgroup is not None else None\n",
    "                    calculate_metrics(y_true_subgroup_disease, y_pred_subgroup_disease, y_pred_proba_subgroup_disease, disease_label=disease)\n",
    "            except Exception:\n",
    "                for disease in diseases:\n",
    "                    y_test_subgroup_disease = y_test_subgroup[disease]\n",
    "                    y_test_preds_disease = model.predict(x_test_subgroup)\n",
    "                    y_test_preds_proba_disease = model.predict_proba(x_test_subgroup)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "                    calculate_metrics(y_test_subgroup_disease, y_test_preds_disease, y_test_preds_proba_disease, disease_label=disease)\n",
    "\n",
    "    # Convert metrics_data to DataFrame and return\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_df = metrics_df.reset_index(drop=True)\n",
    "    print(metrics_df.columns) \n",
    "    return metrics_df\n",
    "\n",
    "def plot_metrics(metrics_df, metric_name, modelname, trainingsize):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 14))  # Größere Figur\n",
    "    fig.suptitle(f\"{metric_name} Comparison Across Dimensions\", fontsize=18)\n",
    "    fig.text(0.5, 0.94, f\"Trained with model: {modelname} on the following training size: {trainingsize}\", \n",
    "             ha='center', fontsize=12, color=\"gray\")\n",
    "\n",
    "    palette = [\"#3498DB\", \"#FFC300\", \"#2ECC71\", \"#E74C3C\"]\n",
    "# Plot 1: Metric by Disease for each Sex\n",
    "    sex_data = metrics_df[metrics_df['Metric Dimension'] == 'sex']\n",
    "    sns.barplot(data=sex_data, x='Disease', y=metric_name, hue='Subgroup', ax=axes[0, 0], palette=palette[:2])\n",
    "    axes[0, 0].set_title(f\"{metric_name} by Disease and Sex\", fontsize=14)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=90)\n",
    "    \n",
    "    # Plot 2: Metric by Disease for each Insurance_Type\n",
    "    insurance_data = metrics_df[metrics_df['Metric Dimension'] == 'insurance_type']\n",
    "    sns.barplot(data=insurance_data, x='Disease', y=metric_name, hue='Subgroup', ax=axes[0, 1], palette=palette[:3])\n",
    "    axes[0, 1].set_title(f\"{metric_name} by Disease and Insurance Type\", fontsize=14)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=90)  \n",
    "    \n",
    "    # Plot 3: Metric by Disease for each Race\n",
    "    race_data = metrics_df[metrics_df['Metric Dimension'] == 'race']\n",
    "    sns.barplot(data=race_data, x='Disease', y=metric_name, hue='Subgroup', ax=axes[1, 0], palette=palette[:3])  \n",
    "    axes[1, 0].set_title(f\"{metric_name} by Disease and Race\", fontsize=14)\n",
    "    axes[1, 0].tick_params(axis='x', rotation=90)  \n",
    "    \n",
    "    # Plot 4: Overall Metric by Disease (no subgroups)\n",
    "    overall_disease_data = metrics_df[(metrics_df['Metric Dimension'] == 'all') & (metrics_df['Disease'] != 'Overall')]\n",
    "    overall_metric = metrics_df[(metrics_df['Metric Dimension'] == 'all') & (metrics_df['Disease'] == 'Overall')]\n",
    "    # Add the overall metric as a new row in the DataFrame for plotting\n",
    "    if not overall_metric.empty:\n",
    "        overall_row = pd.DataFrame({\n",
    "            'Disease': ['Overall'], \n",
    "            metric_name: [overall_metric[metric_name].values[0]]  # Accuracy over all diseases\n",
    "        })\n",
    "        overall_disease_data = pd.concat([overall_disease_data, overall_row], ignore_index=True)\n",
    "\n",
    "    # Plot the data including the \"Overall\" bar\n",
    "    sns.barplot(data=overall_disease_data, x='Disease', y=metric_name, ax=axes[1, 1], color=palette[0])\n",
    "    axes[1, 1].set_title(f\"{metric_name} by Disease (Overall)\", fontsize=14)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=90)\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.3)  \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.92])  \n",
    "    plt.savefig(f\"ml_plots/{metric_name}_{modelname}_{trainingsize}_comparison.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "training_size = 2000\n",
    "use_pca = True        \n",
    "n_components = 10     \n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False,      \n",
    "    eval_metric='logloss',   \n",
    "    learning_rate=0.1,\n",
    "    n_estimators=30,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_metrics = train_model(\n",
    "    x_train=x_train.iloc[:training_size], \n",
    "    y_train=y_train.iloc[:training_size], \n",
    "    x_test=x_test, \n",
    "    y_test=y_test, \n",
    "    model=xgb_model, \n",
    "    metric_dimensions=[\"sex\", \"race\", \"insurance_type\"],\n",
    "    n_components=n_components \n",
    ")\n",
    "\n",
    "# rcf = RandomForestClassifier(n_estimators=30, random_state=42)\n",
    "# rcf_metrics = train_model(\n",
    "#     x_train=x_train.iloc[:training_size], \n",
    "#     y_train=y_train.iloc[:training_size], \n",
    "#     x_test=x_test, \n",
    "#     y_test=y_test, \n",
    "#     model=rcf, \n",
    "#     metric_dimensions=[\"sex\", \"race\", \"insurance_type\"],\n",
    "#     n_components=n_components   \n",
    "# )\n",
    "\n",
    "# # Naive Bayes\n",
    "# naive_bayes = MultinomialNB()\n",
    "# nb_metrics = train_model(\n",
    "#     x_train=x_train.iloc[:training_size], \n",
    "#     y_train=y_train.iloc[:training_size], \n",
    "#     x_test=x_test, \n",
    "#     y_test=y_test, \n",
    "#     model=naive_bayes, \n",
    "#     metric_dimensions=[\"sex\", \"race\", \"insurance_type\"],\n",
    "#     n_components=None  \n",
    "# )\n",
    "\n",
    "# # Gradient Boosting\n",
    "# xgb = GradientBoostingClassifier(learning_rate=0.1, n_estimators=30, verbose=1, random_state=42, n_iter_no_change=5)\n",
    "# xgb_metrics = train_model(\n",
    "#     x_train=x_train.iloc[:training_size], \n",
    "#     y_train=y_train.iloc[:training_size], \n",
    "#     x_test=x_test, \n",
    "#     y_test=y_test, \n",
    "#     model=xgb, \n",
    "#     metric_dimensions=[\"sex\", \"race\", \"insurance_type\"],\n",
    "#     n_components=n_components   \n",
    "# )\n",
    "\n",
    "# # Decision Tree\n",
    "# dct = DecisionTreeClassifier(random_state=42)\n",
    "# dct_metrics = train_model(\n",
    "#     x_train=x_train.iloc[:training_size], \n",
    "#     y_train=y_train.iloc[:training_size], \n",
    "#     x_test=x_test, \n",
    "#     y_test=y_test, \n",
    "#     model=dct, \n",
    "#     metric_dimensions=[\"sex\", \"race\", \"insurance_type\"],\n",
    "#     n_components=n_components \n",
    "# )\n",
    "\n",
    "# Plotting\n",
    "for metric in ['Accuracy', 'F1 Score', 'Recall', 'Precision','AUC']:\n",
    "    #plot_metrics(rcf_metrics, metric, \"RandomForest_with_PCA\" , str(training_size))\n",
    "    #plot_metrics(nb_metrics, metric, \"NaiveBayes\", str(training_size))\n",
    "    plot_metrics(xgb_metrics, metric, \"GradientBoosting_with_PCA\" , str(training_size))\n",
    "    # plot_metrics(dct_metrics, metric, \"DecisionTree_with_PCA\", str(training_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_metrics.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_delta_auc(metrics_df):\n",
    "\n",
    "    df_agg = metrics_df[metrics_df[\"Metric Dimension\"] != \"all\"].groupby([\"Metric Dimension\", \"Disease\"], as_index=False).apply(\n",
    "        lambda x: pd.Series({\n",
    "            \"delta_AUC\": (np.max(x[\"AUC\"]) - np.min(x[\"AUC\"])),\n",
    "            \"eqOdds\": 1/2 * (np.max(x[\"Recall\"]) - np.min(x[\"Recall\"]) ) + 1/2 * (np.max(x[\"FPR\"]) - np.min(x[\"FPR\"]) )\n",
    "        })\n",
    "        \n",
    "    )\n",
    "    \n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "fairness_metrics = calculate_delta_auc(xgb_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique metric dimensions\n",
    "metric_dimensions = fairness_metrics['Metric Dimension'].unique()\n",
    "\n",
    "# Set up the plot\n",
    "fig, axes = plt.subplots(1, len(metric_dimensions), figsize=(15, 6), sharey=True)\n",
    "fig.suptitle(\"Delta AUC per Disease by Metric Dimension\", fontsize=16)\n",
    "\n",
    "# Iterate over each metric dimension to create a subplot\n",
    "for i, metric_dim in enumerate(metric_dimensions):\n",
    "    # Filter the DataFrame for the current metric dimension\n",
    "    subset = fairness_metrics[fairness_metrics['Metric Dimension'] == metric_dim]\n",
    "    \n",
    "    # Plot the delta_AUC for each disease within the current metric dimension\n",
    "    axes[i].bar(subset['Disease'], subset['delta_AUC'], color='skyblue', edgecolor='black')\n",
    "    axes[i].set_title(f\"Metric Dimension: {metric_dim}\")\n",
    "    axes[i].set_xlabel(\"Disease\")\n",
    "    axes[i].tick_params(axis='x', rotation=90)  # Rotate x-axis labels for readability\n",
    "\n",
    "# Set common y-axis label and layout adjustments\n",
    "axes[0].set_ylabel(\"Delta AUC\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to make space for title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique metric dimensions\n",
    "metric_dimensions = fairness_metrics['Metric Dimension'].unique()\n",
    "\n",
    "# Set up the plot\n",
    "fig, axes = plt.subplots(1, len(metric_dimensions), figsize=(15, 6), sharey=True)\n",
    "fig.suptitle(\"Equalized Odds Deltas per Disease by Metric Dimension\", fontsize=16)\n",
    "\n",
    "# Iterate over each metric dimension to create a subplot\n",
    "for i, metric_dim in enumerate(metric_dimensions):\n",
    "    # Filter the DataFrame for the current metric dimension\n",
    "    subset = fairness_metrics[fairness_metrics['Metric Dimension'] == metric_dim]\n",
    "    \n",
    "    # Plot the eqOdds for each disease within the current metric dimension\n",
    "    axes[i].bar(subset['Disease'], subset['eqOdds'], color='skyblue', edgecolor='black')\n",
    "    axes[i].set_title(f\"Metric Dimension: {metric_dim}\")\n",
    "    axes[i].set_xlabel(\"Disease\")\n",
    "    axes[i].tick_params(axis='x', rotation=90)  # Rotate x-axis labels for readability\n",
    "\n",
    "# Set common y-axis label and layout adjustments\n",
    "axes[0].set_ylabel(\"EqOdds Delta\")\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to make space for title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, average_precision_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Liste der Krankheiten\n",
    "diseases = ['Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', \n",
    "            'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', \n",
    "            'Pleural Effusion', 'Pleural Other', 'Fracture']\n",
    "\n",
    "def train_model(x_train, y_train, x_test, y_test, model, metric_dimensions=[], columns_to_drop=[], n_components=None):\n",
    "    multi_output_model = MultiOutputClassifier(model)\n",
    "    x_test = x_test.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    x_train_subset = x_train.drop(columns=columns_to_drop, errors='ignore')\n",
    "    x_test_subset = x_test.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Apply PCA if wanted\n",
    "    if n_components:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        x_train_subset = pca.fit_transform(x_train_subset)\n",
    "        x_test_subset = pca.transform(x_test_subset)\n",
    "        print(f\"PCA used with {n_components} components.\")\n",
    "    else:\n",
    "        print(\"PCA not used.\")\n",
    "\n",
    "    metrics_data = {\n",
    "        'Metric Dimension': [],\n",
    "        'Subgroup': [],\n",
    "        'Disease': [],\n",
    "        'Accuracy': [],\n",
    "        'F1 Score': [],\n",
    "        'Recall': [],  # = True Positive Rate\n",
    "        'FPR': [],\n",
    "        'Precision': [],\n",
    "        'AUCPR': [],\n",
    "    }\n",
    "\n",
    "    def calculate_metrics(y_true, y_pred, y_pred_proba=None, disease_label=\"Overall\"):\n",
    "        \"\"\" Helper function to calculate metrics and add them to metrics_data \"\"\"\n",
    "        metrics_data['Metric Dimension'].append(metric_dim)\n",
    "        metrics_data['Subgroup'].append(subgroup)\n",
    "        metrics_data['Disease'].append(disease_label)\n",
    "        metrics_data['Accuracy'].append(accuracy_score(y_true, y_pred))\n",
    "        metrics_data['F1 Score'].append(f1_score(y_true, y_pred, average='micro'))\n",
    "        metrics_data['Recall'].append(recall_score(y_true, y_pred, average='micro'))\n",
    "        metrics_data['Precision'].append(precision_score(y_true, y_pred, average='micro'))\n",
    "        \n",
    "        if y_pred_proba is not None:\n",
    "            try:\n",
    "                aucpr = average_precision_score(y_true, y_pred_proba)\n",
    "            except ValueError: \n",
    "                aucpr = None\n",
    "            metrics_data['AUCPR'].append(aucpr)\n",
    "        else:\n",
    "            metrics_data['AUCPR'].append(None)\n",
    "\n",
    "        # Calculate FPR\n",
    "        if isinstance(y_true, pd.Series):  # Single-label case\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            if cm.shape == (2, 2):\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "                fpr = fp / (fp + tn) if (fp + tn) > 0 else None\n",
    "            else:\n",
    "                fpr = None\n",
    "            metrics_data['FPR'].append(fpr)\n",
    "        else:  # Multi-label case\n",
    "            if disease_label == \"Overall\":\n",
    "                fprs = []\n",
    "                for c in y_true.columns:\n",
    "                    cm = confusion_matrix(y_true[c], y_pred[c])\n",
    "                    if cm.shape == (2, 2):\n",
    "                        tn, fp, fn, tp = cm.ravel()\n",
    "                        fpr = fp / (fp + tn) if (fp + tn) > 0 else None\n",
    "                        fprs.append(fpr)\n",
    "                overall_fpr = np.nanmean(fprs) if fprs else None  # Mean FPR across all diseases\n",
    "                metrics_data[\"FPR\"].append(overall_fpr)\n",
    "            else:\n",
    "                # For individual diseases, append single FPR\n",
    "                cm = confusion_matrix(y_true, y_pred)\n",
    "                if cm.shape == (2, 2):\n",
    "                    tn, fp, fn, tp = cm.ravel()\n",
    "                    fpr = fp / (fp + tn) if (fp + tn) > 0 else None\n",
    "                else:\n",
    "                    fpr = None\n",
    "                metrics_data[\"FPR\"].append(fpr)\n",
    "\n",
    "    try:\n",
    "        multi_output_model.fit(x_train_subset, y_train)\n",
    "        y_test_preds = pd.DataFrame(multi_output_model.predict(x_test_subset), columns=diseases)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            # multi_output_model.predict_proba returns a list of arrays, one for each class\n",
    "            y_test_preds_proba = pd.DataFrame({disease: probs[:, 1] for disease, probs in zip(diseases, multi_output_model.predict_proba(x_test_subset))}) # Dataframe with probabilities \n",
    "        else:\n",
    "            y_test_preds_proba = None\n",
    "\n",
    "        metric_dim, subgroup = \"all\", \"all\"\n",
    "        calculate_metrics(y_test, y_test_preds, y_test_preds_proba)\n",
    "\n",
    "        # Calculate metrics for each disease individually\n",
    "        for disease in diseases:\n",
    "            y_true_disease = y_test[disease]\n",
    "            y_pred_disease = y_test_preds[disease]\n",
    "            y_pred_proba_disease = y_test_preds_proba[disease] if y_test_preds_proba is not None else None\n",
    "            calculate_metrics(y_true_disease, y_pred_disease, y_pred_proba_disease, disease_label=disease)\n",
    "    except ValueError:\n",
    "        print(\"Multi-output not supported. Training each disease separately.\")\n",
    "        for disease in diseases:\n",
    "            model.fit(x_train_subset, y_train[disease])\n",
    "            y_test_preds = model.predict(x_test_subset)\n",
    "            y_test_preds_proba = model.predict_proba(x_test_subset)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "            metric_dim, subgroup = \"all\", \"all\"\n",
    "            calculate_metrics(y_test[disease], y_test_preds, y_test_preds_proba, disease_label=disease)\n",
    "\n",
    "    # Calculate metrics for each dimension in `metric_dimensions` - in our case mostly sex, race, and insurance_type\n",
    "    for metric_dim in metric_dimensions:\n",
    "        for subgroup in x_test[metric_dim].unique():  # looks at every unique subgroup e.g. female and male \n",
    "            mask = (x_test[metric_dim] == subgroup)  # here we only look at the rows where the subgroup e.g. female is True\n",
    "            x_test_subgroup = x_test_subset[mask]\n",
    "            y_test_subgroup = y_test.loc[mask]\n",
    "            try:\n",
    "                y_test_preds_subgroup = y_test_preds.loc[mask]  # filter the predictions we made before just with that one subgroup\n",
    "                y_test_preds_proba_subgroup = y_test_preds_proba.loc[mask] if y_test_preds_proba is not None else None  # filters also if applicable the y_test_pred_proba for that subgroup and line?\n",
    "                calculate_metrics(y_test_subgroup, y_test_preds_subgroup, y_test_preds_proba_subgroup)\n",
    "                for disease in diseases:\n",
    "                    y_true_subgroup_disease = y_test_subgroup[disease]\n",
    "                    y_pred_subgroup_disease = y_test_preds_subgroup[disease]\n",
    "                    y_pred_proba_subgroup_disease = y_test_preds_proba_subgroup[disease] if y_test_preds_proba_subgroup is not None else None\n",
    "                    calculate_metrics(y_true_subgroup_disease, y_pred_subgroup_disease, y_pred_proba_subgroup_disease, disease_label=disease)\n",
    "            except Exception:\n",
    "                for disease in diseases:\n",
    "                    y_test_subgroup_disease = y_test_subgroup[disease]\n",
    "                    y_test_preds_disease = model.predict(x_test_subgroup)\n",
    "                    y_test_preds_proba_disease = model.predict_proba(x_test_subgroup)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "                    calculate_metrics(y_test_subgroup_disease, y_test_preds_disease, y_test_preds_proba_disease, disease_label=disease)\n",
    "\n",
    "    # Convert metrics_data to DataFrame and return\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    metrics_df = metrics_df.reset_index(drop=True)\n",
    "    print(metrics_df.columns) \n",
    "    return metrics_df\n",
    "\n",
    "def plot_metrics(metrics_df, metric_name, modelname, trainingsize):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 14))  # Größere Figur\n",
    "    fig.suptitle(f\"{metric_name} Comparison Across Dimensions\", fontsize=18)\n",
    "    fig.text(0.5, 0.94, f\"Trained with model: {modelname} on the following training size: {trainingsize}\", \n",
    "             ha='center', fontsize=12, color=\"gray\")\n",
    "\n",
    "    palette = [\"#3498DB\", \"#FFC300\", \"#2ECC71\", \"#E74C3C\"]\n",
    "    \n",
    "    # Plot 1: Metric by Disease for each Sex\n",
    "    sex_data = metrics_df[metrics_df['Metric Dimension'] == 'sex']\n",
    "    sns.barplot(data=sex_data, x='Disease', y=metric_name, hue='Subgroup', ax=axes[0, 0], palette=palette[:2])\n",
    "    axes[0, 0].set_title(f\"{metric_name} by Disease and Sex\", fontsize=14)\n",
    "    axes[0, 0].tick_params(axis='x', rotation=90)\n",
    "    \n",
    "    # Plot 2: Metric by Disease for each Insurance_Type\n",
    "    insurance_data = metrics_df[metrics_df['Metric Dimension'] == 'insurance_type']\n",
    "    sns.barplot(data=insurance_data, x='Disease', y=metric_name, hue='Subgroup', ax=axes[0, 1], palette=palette[:3])\n",
    "    axes[0, 1].set_title(f\"{metric_name} by Disease and Insurance Type\", fontsize=14)\n",
    "    axes[0, 1].tick_params(axis='x', rotation=90)  \n",
    "    \n",
    "    # Plot 3: Metric by Disease for each Race\n",
    "    race_data = metrics_df[metrics_df['Metric Dimension'] == 'race']\n",
    "    sns.barplot(data=race_data, x='Disease', y=metric_name, hue='Subgroup', ax=axes[1, 0], palette=palette[:3])  \n",
    "    axes[1, 0].set_title(f\"{metric_name} by Disease and Race\", fontsize=14)\n",
    "    axes[1, 0].tick_params(axis='x', rotation=90)  \n",
    "    \n",
    "    # Plot 4: Overall Metric by Disease (no subgroups)\n",
    "    overall_disease_data = metrics_df[(metrics_df['Metric Dimension'] == 'all') & (metrics_df['Disease'] != 'Overall')]\n",
    "    overall_metric = metrics_df[(metrics_df['Metric Dimension'] == 'all') & (metrics_df['Disease'] == 'Overall')]\n",
    "    # Add the overall metric as a new row in the DataFrame for plotting\n",
    "    if not overall_metric.empty:\n",
    "        overall_row = pd.DataFrame({\n",
    "            'Disease': ['Overall'], \n",
    "            metric_name: [overall_metric[metric_name].values[0]]  # Accuracy over all diseases\n",
    "        })\n",
    "        overall_disease_data = pd.concat([overall_disease_data, overall_row], ignore_index=True)\n",
    "\n",
    "    # Plot the data including the \"Overall\" bar\n",
    "    sns.barplot(data=overall_disease_data, x='Disease', y=metric_name, ax=axes[1, 1], color=palette[0])\n",
    "    axes[1, 1].set_title(f\"{metric_name} by Disease (Overall)\", fontsize=14)\n",
    "    axes[1, 1].tick_params(axis='x', rotation=90)\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.3)  \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.92])  \n",
    "    plt.savefig(f\"ml_plots/{metric_name}_{modelname}_{trainingsize}_comparison.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Einstellungen\n",
    "training_size = 2000\n",
    "use_pca = True        \n",
    "n_components = 10     \n",
    "\n",
    "# XGBoost-Modell konfigurieren\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False,      \n",
    "    eval_metric='logloss',   \n",
    "    learning_rate=0.1,\n",
    "    n_estimators=30,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Trainiere das Modell und berechne die Metriken\n",
    "xgb_metrics = train_model(\n",
    "    x_train=x_train.iloc[:training_size], \n",
    "    y_train=y_train.iloc[:training_size], \n",
    "    x_test=x_test, \n",
    "    y_test=y_test, \n",
    "    model=xgb_model, \n",
    "    metric_dimensions=[\"sex\", \"race\", \"insurance_type\"],\n",
    "    n_components=n_components \n",
    ")\n",
    "\n",
    "# Optional: Andere Modelle (auskommentiert)\n",
    "# rcf = RandomForestClassifier(n_estimators=30, random_state=42)\n",
    "# rcf_metrics = train_model(\n",
    "#     x_train=x_train.iloc[:training_size], \n",
    "#     y_train=y_train.iloc[:training_size], \n",
    "#     x_test=x_test, \n",
    "#     y_test=y_test, \n",
    "#     model=rcf, \n",
    "#     metric_dimensions=[\"sex\", \"race\", \"insurance_type\"],\n",
    "#     n_components=n_components   \n",
    "# )\n",
    "\n",
    "# # Naive Bayes\n",
    "# naive_bayes = MultinomialNB()\n",
    "# nb_metrics = train_model(\n",
    "#     x_train=x_train.iloc[:training_size], \n",
    "#     y_train=y_train.iloc[:training_size], \n",
    "#     x_test=x_test, \n",
    "#     y_test=y_test, \n",
    "#     model=naive_bayes, \n",
    "#     metric_dimensions=[\"sex\", \"race\", \"insurance_type\"],\n",
    "#     n_components=None  \n",
    "# )\n",
    "\n",
    "# # Gradient Boosting\n",
    "# xgb = GradientBoostingClassifier(learning_rate=0.1, n_estimators=30, verbose=1, random_state=42, n_iter_no_change=5)\n",
    "# xgb_metrics = train_model(\n",
    "#     x_train=x_train.iloc[:training_size], \n",
    "#     y_train=y_train.iloc[:training_size], \n",
    "#     x_test=x_test, \n",
    "#     y_test=y_test, \n",
    "#     model=xgb, \n",
    "#     metric_dimensions=[\"sex\", \"race\", \"insurance_type\"],\n",
    "#     n_components=n_components   \n",
    "# )\n",
    "\n",
    "# # Decision Tree\n",
    "# dct = DecisionTreeClassifier(random_state=42)\n",
    "# dct_metrics = train_model(\n",
    "#     x_train=x_train.iloc[:training_size], \n",
    "#     y_train=y_train.iloc[:training_size], \n",
    "#     x_test=x_test, \n",
    "#     y_test=y_test, \n",
    "#     model=dct, \n",
    "#     metric_dimensions=[\"sex\", \"race\", \"insurance_type\"],\n",
    "#     n_components=n_components \n",
    "# )\n",
    "\n",
    "# Plotting der Metriken\n",
    "for metric in ['Accuracy', 'F1 Score', 'Recall', 'Precision', 'AUCPR']:\n",
    "    plot_metrics(xgb_metrics, metric, \"XGBoost_with_PCA\", str(training_size))\n",
    "    # Andere Modelle plotten, falls aktiviert:\n",
    "    # plot_metrics(rcf_metrics, metric, \"RandomForest_with_PCA\" , str(training_size))\n",
    "    # plot_metrics(nb_metrics, metric, \"NaiveBayes\", str(training_size))\n",
    "    # plot_metrics(dct_metrics, metric, \"DecisionTree_with_PCA\", str(training_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitigate-bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
